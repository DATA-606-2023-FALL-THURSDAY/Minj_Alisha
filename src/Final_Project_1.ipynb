{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aef5092",
   "metadata": {},
   "source": [
    "# **PhishGuard: Phishing Website Detection and Classification Using URL & DNS Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f22a55",
   "metadata": {},
   "source": [
    "## Author Information\n",
    "\n",
    "- Author_name = \"Alisha Minj\"\n",
    "- Affiliation = \"UMBC Data Science Master’s Degree Capstone\"\n",
    "- Github_link = \"https://github.com/DATA-606-2023-FALL-THURSDAY/Minj_Alisha\"\n",
    "- Linkedin_link = \"https://www.linkedin.com/in/alisha-minj\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac07f1",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "\n",
    "The digital landscape, though filled with opportunities, is riddled with threats that pose significant challenges to individual and institutional security. One such prevalent threat is phishing. Phishing websites, designed to deceive and extract confidential information from unsuspecting users, leverage the trust that is typically associated with legitimate domains. As the vastness and complexity of the internet grow, manual detection and mitigation of such threats become increasingly untenable. This project, \"PhishGuard\", endeavors to bridge this gap by employing URL and DNS features to robustly detect and classify domains based on their potential malicious intent.\n",
    "\n",
    "The creation and deployment of an effective detection model holds paramount importance in today's digital age. It extends benefits not only to individual users but also to businesses, organizations, and governments. By proactively identifying and classifying potential threats, we can significantly reduce the risk of data breaches and other security compromises, fostering a safer digital environment for all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05b289",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "Two primary sources drive this project's dataset:\n",
    "1. **PhishTank**: An open-source service that provides an hourly updated list of phishing URLs. \n",
    "    - [PhishTank Data](https://www.phishtank.com/developer_info.php)\n",
    "    \n",
    "    \n",
    "2. **University of New Brunswick**: They offer a collection encompassing benign, spam, phishing, malware, and defacement URLs. \n",
    "    - [UNB Dataset](https://www.unb.ca/cic/datasets/url-2016.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de188f",
   "metadata": {},
   "source": [
    "## **Data Collection and Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dee1ba",
   "metadata": {},
   "source": [
    "Data plays a crucial role in building efficient machine learning models. For this project, we rely on two primary sources to obtain our datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75bccc",
   "metadata": {},
   "source": [
    "## **Phishing URLs**\n",
    "     \n",
    "Source: PhishTank\n",
    "\n",
    "PhishTank, an open community project, facilitates users to identify and report phishing URLs, ensuring that a comprehensive, updated list of phishing URLs is available for researchers and analysts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63fefb",
   "metadata": {},
   "source": [
    "- **Loading Phishing URLs:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0946b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67e72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhishTank provides an up-to-date dataset of live phishing URLs.\n",
    "# For this project, we're downloading the latest set of valid phishing URLs.\n",
    "#!wget http://data.phishtank.com/data/online-valid.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458bbb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phish_id</th>\n",
       "      <th>url</th>\n",
       "      <th>phish_detail_url</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>verified</th>\n",
       "      <th>verification_time</th>\n",
       "      <th>online</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6557033</td>\n",
       "      <td>http://u1047531.cp.regruhosting.ru/acces-inges...</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-05-09T22:01:43+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-05-09T22:03:07+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6557032</td>\n",
       "      <td>http://hoysalacreations.com/wp-content/plugins...</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-05-09T22:01:37+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-05-09T22:03:07+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6557011</td>\n",
       "      <td>http://www.accsystemprblemhelp.site/checkpoint...</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-05-09T21:54:31+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-05-09T21:55:38+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6557010</td>\n",
       "      <td>http://www.accsystemprblemhelp.site/login_atte...</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-05-09T21:53:48+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-05-09T21:54:34+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6557009</td>\n",
       "      <td>https://firebasestorage.googleapis.com/v0/b/so...</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-05-09T21:49:27+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-05-09T21:51:24+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Microsoft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phish_id                                                url  \\\n",
       "0   6557033  http://u1047531.cp.regruhosting.ru/acces-inges...   \n",
       "1   6557032  http://hoysalacreations.com/wp-content/plugins...   \n",
       "2   6557011  http://www.accsystemprblemhelp.site/checkpoint...   \n",
       "3   6557010  http://www.accsystemprblemhelp.site/login_atte...   \n",
       "4   6557009  https://firebasestorage.googleapis.com/v0/b/so...   \n",
       "\n",
       "                                    phish_detail_url  \\\n",
       "0  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "1  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "2  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "3  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "4  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "\n",
       "             submission_time verified          verification_time online  \\\n",
       "0  2020-05-09T22:01:43+00:00      yes  2020-05-09T22:03:07+00:00    yes   \n",
       "1  2020-05-09T22:01:37+00:00      yes  2020-05-09T22:03:07+00:00    yes   \n",
       "2  2020-05-09T21:54:31+00:00      yes  2020-05-09T21:55:38+00:00    yes   \n",
       "3  2020-05-09T21:53:48+00:00      yes  2020-05-09T21:54:34+00:00    yes   \n",
       "4  2020-05-09T21:49:27+00:00      yes  2020-05-09T21:51:24+00:00    yes   \n",
       "\n",
       "      target  \n",
       "0      Other  \n",
       "1      Other  \n",
       "2   Facebook  \n",
       "3   Facebook  \n",
       "4  Microsoft  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of the dataset\n",
    "data_url = \"https://raw.githubusercontent.com/DATA-606-2023-FALL-THURSDAY/Minj_Alisha/main/data/PhishingData.csv\"\n",
    "\n",
    "# Loading the data into a pandas DataFrame\n",
    "phish_data = pd.read_csv(data_url)\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "phish_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4565dd",
   "metadata": {},
   "source": [
    "The displayed table offers a glimpse of the dataset structure, illustrating features such as the unique identifier (phish_id), the URL in question (url), the detailed link for verification (phish_detail_url), the submission time, verification attributes, and the targeted brand or entity (target).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78746c9",
   "metadata": {},
   "source": [
    "## **Legitimate URLs**\n",
    "\n",
    "Source: University of New Brunswick\n",
    "\n",
    "For the legitimate URLs, we rely on a dataset from the University of New Brunswick, which has been collected to serve as a counter-part to phishing URLs. These URLs are from trustworthy sites and do not pose any security threat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405f72a",
   "metadata": {},
   "source": [
    "- **Loading Legitimate URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f456e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1337x.to/torrent/1110018/Blackhat-2015-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://1337x.to/torrent/1122940/Blackhat-2015-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://1337x.to/torrent/1124395/Fast-and-Furio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://1337x.to/torrent/1145504/Avengers-Age-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://1337x.to/torrent/1160078/Avengers-age-o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  http://1337x.to/torrent/1110018/Blackhat-2015-...\n",
       "1  http://1337x.to/torrent/1122940/Blackhat-2015-...\n",
       "2  http://1337x.to/torrent/1124395/Fast-and-Furio...\n",
       "3  http://1337x.to/torrent/1145504/Avengers-Age-o...\n",
       "4  http://1337x.to/torrent/1160078/Avengers-age-o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Benign URLs Data from the URL\n",
    "# URL of the benign dataset\n",
    "benign_data_url = \"https://raw.githubusercontent.com/DATA-606-2023-FALL-THURSDAY/Minj_Alisha/main/data/BenignURLs.csv\"\n",
    "\n",
    "# Loading the benign data into a pandas DataFrame\n",
    "legit_data = pd.read_csv(benign_data_url)\n",
    "\n",
    "# Renaming the column to maintain consistency\n",
    "legit_data.columns = ['url']\n",
    "\n",
    "# Displaying the first few rows of the benign dataset\n",
    "legit_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f84a2",
   "metadata": {},
   "source": [
    "The dataset consists of URLs that have been flagged as non-malicious and serve as a standard for legitimate online entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4500a",
   "metadata": {},
   "source": [
    "## **Data Overview**\n",
    "\n",
    "Once we've loaded the datasets, it's beneficial to get a preliminary understanding of our data.\n",
    "\n",
    "A preliminary understanding of the datasets is achieved by examining the number of URLs and the distribution between phishing and legitimate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6e9f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Phishing URLs Loaded: 14858\n",
      "Total Legitimate URLs Loaded: 35377\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Phishing URLs Loaded: {len(phish_data)}\")\n",
    "#print(f\"Total Phishing URLs Selected for Analysis: {len(phishing_subset)}\")\n",
    "print(f\"Total Legitimate URLs Loaded: {len(legit_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e13fc8",
   "metadata": {},
   "source": [
    "The dataset contains more than twice as many legitimate URLs compared to phishing URLs, indicating an imbalance that may need addressing in the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538caa5b",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "From the output, it's evident we have loaded 14,858 phishing URLs and a more substantial 35,377 legitimate URLs. This discrepancy highlights the challenge of managing imbalanced datasets. While we have a vast volume of URLs at our disposal, the differential distribution emphasizes the necessity to consider balanced datasets when moving to the modeling phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2e903",
   "metadata": {},
   "source": [
    "**Data Quality Check**\n",
    "\n",
    "Before diving into feature extraction, it's essential to ensure the quality and integrity of the data.\n",
    "\n",
    "Assessing the quality and reliability of our data is pivotal before any intensive analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d070fce",
   "metadata": {},
   "source": [
    "- **Basic Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff99ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Phishing URLs: 14858\n",
      "Unique Phishing URLs: 14855\n"
     ]
    }
   ],
   "source": [
    "# Phishing URLs\n",
    "print(\"Number of Phishing URLs:\", len(phish_data))\n",
    "print(\"Unique Phishing URLs:\", phish_data['url'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96427275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Phishing URLs after removing duplicates: 14855\n",
      "Unique Phishing URLs after removing duplicates: 14855\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicate URLs\n",
    "phish_data.drop_duplicates(subset='url', keep='first', inplace=True)\n",
    "\n",
    "# Verifying the removal\n",
    "print(\"Number of Phishing URLs after removing duplicates:\", len(phish_data))\n",
    "print(\"Unique Phishing URLs after removing duplicates:\", phish_data['url'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cf00d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Legitimate URLs: 35377\n",
      "Unique Legitimate URLs: 35377\n"
     ]
    }
   ],
   "source": [
    "# Legitimate URLs\n",
    "print(\"Number of Legitimate URLs:\", len(legit_data))\n",
    "print(\"Unique Legitimate URLs:\", legit_data['url'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273582e",
   "metadata": {},
   "source": [
    "By examining the total URLs against unique entries, we can identify potential redundancies or duplicates in the data, which are evident in the phishing dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6173b7",
   "metadata": {},
   "source": [
    "- **Missing Values**\n",
    "\n",
    "Ensuring the data's completeness is paramount. Missing or NaN values can disrupt the analysis and modeling process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf32b556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Phishing dataset: phish_id             0\n",
      "url                  0\n",
      "phish_detail_url     0\n",
      "submission_time      0\n",
      "verified             0\n",
      "verification_time    0\n",
      "online               0\n",
      "target               0\n",
      "dtype: int64\n",
      "Missing values in Legitimate dataset: url    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Phishing URLs\n",
    "print(\"Missing values in Phishing dataset:\", phish_data.isnull().sum())\n",
    "\n",
    "# Legitimate URLs\n",
    "print(\"Missing values in Legitimate dataset:\", legit_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255223b0",
   "metadata": {},
   "source": [
    "The datasets appear clean without any missing values, indicating that each URL is accompanied by its respective attributes without any gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb860819",
   "metadata": {},
   "source": [
    "- **Duplicate Values**\n",
    "\n",
    "Redundant entries need to be addressed to maintain the dataset's integrity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e7ac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14855 unique phishing URLs remain. No duplicate values.\n"
     ]
    }
   ],
   "source": [
    "# Phishing URLs\n",
    "initial_count = len(phish_data)\n",
    "phish_data.drop_duplicates(subset=\"url\", keep=False, inplace=True)\n",
    "final_count = len(phish_data)\n",
    "\n",
    "# Indicate the number of unique values and confirm no duplicates\n",
    "print(f\"{final_count} unique phishing URLs remain. No duplicate values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ba8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35377 unique phishing URLs remain. No duplicate values.\n"
     ]
    }
   ],
   "source": [
    "# Legitimate URLs\n",
    "initial_count = len(legit_data)\n",
    "phish_data.drop_duplicates(subset=\"url\", keep=False, inplace=True)\n",
    "final_count = len(legit_data)\n",
    "\n",
    "# Indicate the number of unique values and confirm no duplicates\n",
    "print(f\"{final_count} unique phishing URLs remain. No duplicate values.\")\n",
    "legit_data.drop_duplicates(subset =\"url\", keep = False, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda69f1",
   "metadata": {},
   "source": [
    "- **Data Distribution**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d67041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs: 29710\n"
     ]
    }
   ],
   "source": [
    "# Taking a subset of 14,855 from both datasets to maintain balance\n",
    "phishing_subset = phish_data.sample(n=14855, random_state=42)\n",
    "legit_subset = legit_data.sample(n=14855, random_state=42)\n",
    "\n",
    "print(f\"Total URLs: {len(phishing_subset) + len(legit_subset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b3c43",
   "metadata": {},
   "source": [
    "- **Sample URL Preview**\n",
    "\n",
    "Sampling a few URLs from each dataset provides a firsthand look into the nature of entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64dc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Phishing URLs:\n",
      "       phish_id                                                url  \\\n",
      "13934   5843400     https://apple.com-recaptcha.edgemasterint.com/   \n",
      "4084    6539084  http://clsrockies.com/dropbox-loginclient/drop...   \n",
      "6900    6512675  http://www.testing-adjusting-balancing.com/wp-...   \n",
      "3520    6542447  https://irannail.com/override/classes/assets/b...   \n",
      "5106    6531163  http://rohtolab.com/public/templates/mmp/web/-...   \n",
      "\n",
      "                                        phish_detail_url  \\\n",
      "13934  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "4084   http://www.phishtank.com/phish_detail.php?phis...   \n",
      "6900   http://www.phishtank.com/phish_detail.php?phis...   \n",
      "3520   http://www.phishtank.com/phish_detail.php?phis...   \n",
      "5106   http://www.phishtank.com/phish_detail.php?phis...   \n",
      "\n",
      "                 submission_time verified          verification_time online  \\\n",
      "13934  2018-11-14T13:06:19+00:00      yes  2018-12-23T23:41:07+00:00    yes   \n",
      "4084   2020-04-30T14:03:14+00:00      yes  2020-04-30T14:54:13+00:00    yes   \n",
      "6900   2020-04-18T02:12:36+00:00      yes  2020-04-18T02:39:29+00:00    yes   \n",
      "3520   2020-05-02T10:00:59+00:00      yes  2020-05-03T11:42:41+00:00    yes   \n",
      "5106   2020-04-27T02:08:21+00:00      yes  2020-04-27T02:19:16+00:00    yes   \n",
      "\n",
      "       target  \n",
      "13934   Apple  \n",
      "4084    Other  \n",
      "6900    Other  \n",
      "3520   PayPal  \n",
      "5106    Other  \n",
      "\n",
      "Sample Legitimate URLs:\n",
      "                                                     url\n",
      "18416  http://bdnews24.com/environment/2015/04/09/jap...\n",
      "17969  http://indianexpress.com/article/world/climate...\n",
      "13172  https://medium.com/human-parts/why-a-dutch-mug...\n",
      "19008  http://udn.com/news/story/7321/901410-%E9%A8%8...\n",
      "32776  http://otomoto.pl/oferta/komatsu-pc-240-nlc-8-...\n"
     ]
    }
   ],
   "source": [
    "# Phishing URLs\n",
    "print(\"Sample Phishing URLs:\")\n",
    "print(phish_data.sample(5))\n",
    "\n",
    "# Legitimate URLs\n",
    "print(\"\\nSample Legitimate URLs:\")\n",
    "print(legit_data.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed394c",
   "metadata": {},
   "source": [
    "The displayed samples provide a taste of what the datasets contain, from the structure of URLs to the targeted entities in the case of phishing URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1fbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages for this section\n",
    "from urllib.parse import urlparse,urlencode\n",
    "import ipaddress\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfa14d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 1.Domain of the URL (Domain) \\ndef getDomain(url):  \\n  domain = urlparse(url).netloc\\n  if re.match(r\"^www.\",domain):\\n\\t       domain = domain.replace(\"www.\",\"\")\\n  return domain\\n     \\n\\n# 2.Checks for IP address in URL (Have_IP)\\ndef havingIP(url):\\n  try:\\n    ipaddress.ip_address(url)\\n    ip = 1\\n  except:\\n    ip = 0\\n  return ip\\n\\n# 3.Checks the presence of @ in URL (Have_At)\\ndef haveAtSign(url):\\n  if \"@\" in url:\\n    at = 1    \\n  else:\\n    at = 0    \\n  return at\\n\\n# 4.Finding the length of URL and categorizing (URL_Length)\\ndef getLength(url):\\n  if len(url) < 54:\\n    length = 0            \\n  else:\\n    length = 1            \\n  return length\\n\\n# 5.Gives number of \\'/\\' in URL (URL_Depth)\\ndef getDepth(url):\\n  s = urlparse(url).path.split(\\'/\\')\\n  depth = 0\\n  for j in range(len(s)):\\n    if len(s[j]) != 0:\\n      depth = depth+1\\n  return depth\\n     \\n\\n# 6.Checking for redirection \\'//\\' in the url (Redirection)\\ndef redirection(url):\\n  pos = url.rfind(\\'//\\')\\n  if pos > 6:\\n    if pos > 7:\\n      return 1\\n    else:\\n      return 0\\n  else:\\n    return 0\\n     \\n\\n# 7.Existence of “HTTPS” Token in the Domain Part of the URL (https_Domain)\\ndef httpDomain(url):\\n  domain = urlparse(url).netloc\\n  if \\'https\\' in domain:\\n    return 1\\n  else:\\n    return 0\\n\\n#listing shortening services\\nshortening_services = r\"bit\\\\.ly|goo\\\\.gl|shorte\\\\.st|go2l\\\\.ink|x\\\\.co|ow\\\\.ly|t\\\\.co|tinyurl|tr\\\\.im|is\\\\.gd|cli\\\\.gs|\"                       r\"yfrog\\\\.com|migre\\\\.me|ff\\\\.im|tiny\\\\.cc|url4\\\\.eu|twit\\\\.ac|su\\\\.pr|twurl\\\\.nl|snipurl\\\\.com|\"                       r\"short\\\\.to|BudURL\\\\.com|ping\\\\.fm|post\\\\.ly|Just\\\\.as|bkite\\\\.com|snipr\\\\.com|fic\\\\.kr|loopt\\\\.us|\"                       r\"doiop\\\\.com|short\\\\.ie|kl\\\\.am|wp\\\\.me|rubyurl\\\\.com|om\\\\.ly|to\\\\.ly|bit\\\\.do|t\\\\.co|lnkd\\\\.in|db\\\\.tt|\"                       r\"qr\\\\.ae|adf\\\\.ly|goo\\\\.gl|bitly\\\\.com|cur\\\\.lv|tinyurl\\\\.com|ow\\\\.ly|bit\\\\.ly|ity\\\\.im|q\\\\.gs|is\\\\.gd|\"                       r\"po\\\\.st|bc\\\\.vc|twitthis\\\\.com|u\\\\.to|j\\\\.mp|buzurl\\\\.com|cutt\\\\.us|u\\\\.bb|yourls\\\\.org|x\\\\.co|\"                       r\"prettylinkpro\\\\.com|scrnch\\\\.me|filoops\\\\.info|vzturl\\\\.com|qr\\\\.net|1url\\\\.com|tweez\\\\.me|v\\\\.gd|\"                       r\"tr\\\\.im|link\\\\.zip\\\\.net\"\\n     \\n\\n\\n# 8. Checking for Shortening Services in URL (Tiny_URL)\\ndef tinyURL(url):\\n    match=re.search(shortening_services,url)\\n    if match:\\n        return 1\\n    else:\\n        return 0\\n\\n# 9.Checking for Prefix or Suffix Separated by (-) in the Domain (Prefix/Suffix)\\ndef prefixSuffix(url):\\n    if \\'-\\' in urlparse(url).netloc:\\n        return 1            # phishing\\n    else:\\n        return 0            # legitimate\\n\\n\\n!pip install python-whois\\n\\n# importing required packages for this section\\nimport re\\nfrom bs4 import BeautifulSoup\\nimport whois\\nimport urllib\\nimport urllib.request\\nfrom datetime import datetime\\n     \\n\\n# 11.DNS Record availability (DNS_Record)\\n# obtained in the featureExtraction function itself\\n     \\n\\n# 12.Web traffic (Web_Traffic)\\ndef web_traffic(url):\\n  try:\\n    #Filling the whitespaces in the URL if any\\n    url = urllib.parse.quote(url)\\n    rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\\n        \"REACH\")[\\'RANK\\']\\n    rank = int(rank)\\n  except TypeError:\\n        return 1\\n  if rank <100000:\\n    return 1\\n  else:\\n    return 0\\n\\n# 13.Survival time of domain: The difference between termination time and creation time (Domain_Age)  \\ndef domainAge(domain_name):\\n  creation_date = domain_name.creation_date\\n  expiration_date = domain_name.expiration_date\\n  if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\\n    try:\\n      creation_date = datetime.strptime(creation_date,\\'%Y-%m-%d\\')\\n      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\\n    except:\\n      return 1\\n  if ((expiration_date is None) or (creation_date is None)):\\n      return 1\\n  elif ((type(expiration_date) is list) or (type(creation_date) is list)):\\n      return 1\\n  else:\\n    ageofdomain = abs((expiration_date - creation_date).days)\\n    if ((ageofdomain/30) < 6):\\n      age = 1\\n    else:\\n      age = 0\\n  return age\\n     \\n\\n# 14.End time of domain: The difference between termination time and current time (Domain_End) \\ndef domainEnd(domain_name):\\n  expiration_date = domain_name.expiration_date\\n  if isinstance(expiration_date,str):\\n    try:\\n      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\\n    except:\\n      return 1\\n  if (expiration_date is None):\\n      return 1\\n  elif (type(expiration_date) is list):\\n      return 1\\n  else:\\n    today = datetime.now()\\n    end = abs((expiration_date - today).days)\\n    if ((end/30) < 6):\\n      end = 0\\n    else:\\n      end = 1\\n  return end\\n\\n\\n# importing required packages for this section\\nimport requests\\n\\n# 15. IFrame Redirection (iFrame)\\ndef iframe(response):\\n  if response == \"\":\\n      return 1\\n  else:\\n      if re.findall(r\"[|]\", response.text):\\n          return 0\\n      else:\\n          return 1\\n\\n# 16.Checks the effect of mouse over on status bar (Mouse_Over)\\ndef mouseOver(response): \\n  if response == \"\" :\\n    return 1\\n  else:\\n    if re.findall(\"\", response.text):\\n      return 1\\n    else:\\n      return 0\\n\\n# 17.Checks the status of the right click attribute (Right_Click)\\ndef rightClick(response):\\n  if response == \"\":\\n    return 1\\n  else:\\n    if re.findall(r\"event.button ?== ?2\", response.text):\\n      return 0\\n    else:\\n      return 1\\n\\n\\n# 18.Checks the number of forwardings (Web_Forwards)    \\ndef forwarding(response):\\n  if response == \"\":\\n    return 1\\n  else:\\n    if len(response.history) <= 2:\\n      return 0\\n    else:\\n      return 1\\n     \\n\\n#Function to extract features\\ndef featureExtraction(url,label):\\n\\n  features = []\\n  #Address bar based features (10)\\n  features.append(getDomain(url))\\n  features.append(havingIP(url))\\n  features.append(haveAtSign(url))\\n  features.append(getLength(url))\\n  features.append(getDepth(url))\\n  features.append(redirection(url))\\n  features.append(httpDomain(url))\\n  features.append(tinyURL(url))\\n  features.append(prefixSuffix(url))\\n  \\n  #Domain based features (4)\\n  dns = 0\\n  try:\\n    domain_name = whois.whois(urlparse(url).netloc)\\n  except:\\n    dns = 1\\n\\n  features.append(dns)\\n  features.append(web_traffic(url))\\n  features.append(1 if dns == 1 else domainAge(domain_name))\\n  features.append(1 if dns == 1 else domainEnd(domain_name))\\n  \\n  # HTML & Javascript based features (4)\\n  try:\\n    response = requests.get(url)\\n  except:\\n    response = \"\"\\n  features.append(iframe(response))\\n  features.append(mouseOver(response))\\n  features.append(rightClick(response))\\n  features.append(forwarding(response))\\n  features.append(label)\\n  \\n  return features\\n\\nlegit_subset.shape\\n\\nfor i in range(0, 14855):\\n    url = legit_subset[\\'url\\'].iloc[i]\\n    ...\\n\\n\\nlegit_subset = legit_subset.reset_index(drop=True)\\nfor i in range(0, 14855):\\n    url = legit_subset[\\'url\\'][i]\\n    ...\\n\\n\\nlegi_features = []\\nlabel = 0\\n\\n# Reset the index for the subset dataframe\\nlegit_subset = legit_subset.reset_index(drop=True)\\n\\n# Loop through each URL in the legit_subset dataframe\\nfor i in range(0, 14855):\\n    url = legit_subset[\\'url\\'][i]\\n    try:\\n        # Append the features extracted from the URL to the legi_features list\\n        legi_features.append(featureExtraction(url, label))\\n    except Exception as e:\\n        # If there\\'s an error with extracting features for a specific URL, print the error and continue\\n        print(f\"Error processing URL {url}: {e}\")\\n        continue\\n\\n# You should now have a list of features for each legitimate URL in legi_features\\n\\n\\n#converting the list to dataframe\\nfeature_names = [\\'Domain\\', \\'Have_IP\\', \\'Have_At\\', \\'URL_Length\\', \\'URL_Depth\\',\\'Redirection\\', \\n                      \\'https_Domain\\', \\'TinyURL\\', \\'Prefix/Suffix\\', \\'DNS_Record\\', \\'Web_Traffic\\', \\n                      \\'Domain_Age\\', \\'Domain_End\\', \\'iFrame\\', \\'Mouse_Over\\',\\'Right_Click\\', \\'Web_Forwards\\', \\'Label\\']\\n\\nlegitimate = pd.DataFrame(legi_features, columns= feature_names)\\nlegitimate.head()\\n\\n# Storing the extracted legitimate URLs fatures to csv file\\nlegitimate.to_csv(\\'legitimate.csv\\', index= False)\\n     \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# 1.Domain of the URL (Domain) \n",
    "def getDomain(url):  \n",
    "  domain = urlparse(url).netloc\n",
    "  if re.match(r\"^www.\",domain):\n",
    "\t       domain = domain.replace(\"www.\",\"\")\n",
    "  return domain\n",
    "     \n",
    "\n",
    "# 2.Checks for IP address in URL (Have_IP)\n",
    "def havingIP(url):\n",
    "  try:\n",
    "    ipaddress.ip_address(url)\n",
    "    ip = 1\n",
    "  except:\n",
    "    ip = 0\n",
    "  return ip\n",
    "\n",
    "# 3.Checks the presence of @ in URL (Have_At)\n",
    "def haveAtSign(url):\n",
    "  if \"@\" in url:\n",
    "    at = 1    \n",
    "  else:\n",
    "    at = 0    \n",
    "  return at\n",
    "\n",
    "# 4.Finding the length of URL and categorizing (URL_Length)\n",
    "def getLength(url):\n",
    "  if len(url) < 54:\n",
    "    length = 0            \n",
    "  else:\n",
    "    length = 1            \n",
    "  return length\n",
    "\n",
    "# 5.Gives number of '/' in URL (URL_Depth)\n",
    "def getDepth(url):\n",
    "  s = urlparse(url).path.split('/')\n",
    "  depth = 0\n",
    "  for j in range(len(s)):\n",
    "    if len(s[j]) != 0:\n",
    "      depth = depth+1\n",
    "  return depth\n",
    "     \n",
    "\n",
    "# 6.Checking for redirection '//' in the url (Redirection)\n",
    "def redirection(url):\n",
    "  pos = url.rfind('//')\n",
    "  if pos > 6:\n",
    "    if pos > 7:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "  else:\n",
    "    return 0\n",
    "     \n",
    "\n",
    "# 7.Existence of “HTTPS” Token in the Domain Part of the URL (https_Domain)\n",
    "def httpDomain(url):\n",
    "  domain = urlparse(url).netloc\n",
    "  if 'https' in domain:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "#listing shortening services\n",
    "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net\"\n",
    "     \n",
    "\n",
    "\n",
    "# 8. Checking for Shortening Services in URL (Tiny_URL)\n",
    "def tinyURL(url):\n",
    "    match=re.search(shortening_services,url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 9.Checking for Prefix or Suffix Separated by (-) in the Domain (Prefix/Suffix)\n",
    "def prefixSuffix(url):\n",
    "    if '-' in urlparse(url).netloc:\n",
    "        return 1            # phishing\n",
    "    else:\n",
    "        return 0            # legitimate\n",
    "\n",
    "\n",
    "!pip install python-whois\n",
    "\n",
    "# importing required packages for this section\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "import urllib\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "     \n",
    "\n",
    "# 11.DNS Record availability (DNS_Record)\n",
    "# obtained in the featureExtraction function itself\n",
    "     \n",
    "\n",
    "# 12.Web traffic (Web_Traffic)\n",
    "def web_traffic(url):\n",
    "  try:\n",
    "    #Filling the whitespaces in the URL if any\n",
    "    url = urllib.parse.quote(url)\n",
    "    rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\n",
    "        \"REACH\")['RANK']\n",
    "    rank = int(rank)\n",
    "  except TypeError:\n",
    "        return 1\n",
    "  if rank <100000:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "# 13.Survival time of domain: The difference between termination time and creation time (Domain_Age)  \n",
    "def domainAge(domain_name):\n",
    "  creation_date = domain_name.creation_date\n",
    "  expiration_date = domain_name.expiration_date\n",
    "  if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "    try:\n",
    "      creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "    except:\n",
    "      return 1\n",
    "  if ((expiration_date is None) or (creation_date is None)):\n",
    "      return 1\n",
    "  elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
    "      return 1\n",
    "  else:\n",
    "    ageofdomain = abs((expiration_date - creation_date).days)\n",
    "    if ((ageofdomain/30) < 6):\n",
    "      age = 1\n",
    "    else:\n",
    "      age = 0\n",
    "  return age\n",
    "     \n",
    "\n",
    "# 14.End time of domain: The difference between termination time and current time (Domain_End) \n",
    "def domainEnd(domain_name):\n",
    "  expiration_date = domain_name.expiration_date\n",
    "  if isinstance(expiration_date,str):\n",
    "    try:\n",
    "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "    except:\n",
    "      return 1\n",
    "  if (expiration_date is None):\n",
    "      return 1\n",
    "  elif (type(expiration_date) is list):\n",
    "      return 1\n",
    "  else:\n",
    "    today = datetime.now()\n",
    "    end = abs((expiration_date - today).days)\n",
    "    if ((end/30) < 6):\n",
    "      end = 0\n",
    "    else:\n",
    "      end = 1\n",
    "  return end\n",
    "\n",
    "\n",
    "# importing required packages for this section\n",
    "import requests\n",
    "\n",
    "# 15. IFrame Redirection (iFrame)\n",
    "def iframe(response):\n",
    "  if response == \"\":\n",
    "      return 1\n",
    "  else:\n",
    "      if re.findall(r\"[|]\", response.text):\n",
    "          return 0\n",
    "      else:\n",
    "          return 1\n",
    "\n",
    "# 16.Checks the effect of mouse over on status bar (Mouse_Over)\n",
    "def mouseOver(response): \n",
    "  if response == \"\" :\n",
    "    return 1\n",
    "  else:\n",
    "    if re.findall(\"\", response.text):\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "# 17.Checks the status of the right click attribute (Right_Click)\n",
    "def rightClick(response):\n",
    "  if response == \"\":\n",
    "    return 1\n",
    "  else:\n",
    "    if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "\n",
    "\n",
    "# 18.Checks the number of forwardings (Web_Forwards)    \n",
    "def forwarding(response):\n",
    "  if response == \"\":\n",
    "    return 1\n",
    "  else:\n",
    "    if len(response.history) <= 2:\n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "     \n",
    "\n",
    "#Function to extract features\n",
    "def featureExtraction(url,label):\n",
    "\n",
    "  features = []\n",
    "  #Address bar based features (10)\n",
    "  features.append(getDomain(url))\n",
    "  features.append(havingIP(url))\n",
    "  features.append(haveAtSign(url))\n",
    "  features.append(getLength(url))\n",
    "  features.append(getDepth(url))\n",
    "  features.append(redirection(url))\n",
    "  features.append(httpDomain(url))\n",
    "  features.append(tinyURL(url))\n",
    "  features.append(prefixSuffix(url))\n",
    "  \n",
    "  #Domain based features (4)\n",
    "  dns = 0\n",
    "  try:\n",
    "    domain_name = whois.whois(urlparse(url).netloc)\n",
    "  except:\n",
    "    dns = 1\n",
    "\n",
    "  features.append(dns)\n",
    "  features.append(web_traffic(url))\n",
    "  features.append(1 if dns == 1 else domainAge(domain_name))\n",
    "  features.append(1 if dns == 1 else domainEnd(domain_name))\n",
    "  \n",
    "  # HTML & Javascript based features (4)\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    response = \"\"\n",
    "  features.append(iframe(response))\n",
    "  features.append(mouseOver(response))\n",
    "  features.append(rightClick(response))\n",
    "  features.append(forwarding(response))\n",
    "  features.append(label)\n",
    "  \n",
    "  return features\n",
    "\n",
    "legit_subset.shape\n",
    "\n",
    "for i in range(0, 14855):\n",
    "    url = legit_subset['url'].iloc[i]\n",
    "    ...\n",
    "\n",
    "\n",
    "legit_subset = legit_subset.reset_index(drop=True)\n",
    "for i in range(0, 14855):\n",
    "    url = legit_subset['url'][i]\n",
    "    ...\n",
    "\n",
    "\n",
    "legi_features = []\n",
    "label = 0\n",
    "\n",
    "# Reset the index for the subset dataframe\n",
    "legit_subset = legit_subset.reset_index(drop=True)\n",
    "\n",
    "# Loop through each URL in the legit_subset dataframe\n",
    "for i in range(0, 14855):\n",
    "    url = legit_subset['url'][i]\n",
    "    try:\n",
    "        # Append the features extracted from the URL to the legi_features list\n",
    "        legi_features.append(featureExtraction(url, label))\n",
    "    except Exception as e:\n",
    "        # If there's an error with extracting features for a specific URL, print the error and continue\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# You should now have a list of features for each legitimate URL in legi_features\n",
    "\n",
    "\n",
    "#converting the list to dataframe\n",
    "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection', \n",
    "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic', \n",
    "                      'Domain_Age', 'Domain_End', 'iFrame', 'Mouse_Over','Right_Click', 'Web_Forwards', 'Label']\n",
    "\n",
    "legitimate = pd.DataFrame(legi_features, columns= feature_names)\n",
    "legitimate.head()\n",
    "\n",
    "# Storing the extracted legitimate URLs fatures to csv file\n",
    "legitimate.to_csv('legitimate.csv', index= False)\n",
    "     \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28715200",
   "metadata": {},
   "source": [
    "## **Feature Extraction**\n",
    "\n",
    "Feature extraction is a pivotal step in the data preprocessing pipeline, especially when dealing with URLs. Unlike structured data, URLs come in a textual format which isn't directly consumable by most machine learning models. By extracting meaningful features from these URLs, we can transform this unstructured data into a structured format, making it suitable for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25787564",
   "metadata": {},
   "source": [
    "- **URL Length**\n",
    "\n",
    "The length of a URL can be a simple yet effective feature. Phishing URLs tend to be longer than legitimate ones as attackers often embed malicious domains within sub-domains to disguise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24f228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_data['url_length'] = phish_data['url'].apply(len)\n",
    "legit_data['url_length'] = legit_data['url'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf014d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing Data\n",
      "                                                 url  url_length\n",
      "0  http://u1047531.cp.regruhosting.ru/acces-inges...          66\n",
      "1  http://hoysalacreations.com/wp-content/plugins...          88\n",
      "2  http://www.accsystemprblemhelp.site/checkpoint...          50\n",
      "3  http://www.accsystemprblemhelp.site/login_atte...          67\n",
      "4  https://firebasestorage.googleapis.com/v0/b/so...         137\n"
     ]
    }
   ],
   "source": [
    "print('Phishing Data')\n",
    "print(phish_data[['url', 'url_length']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e0c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate Data\n",
      "                                                 url  url_length\n",
      "0  http://1337x.to/torrent/1110018/Blackhat-2015-...          83\n",
      "1  http://1337x.to/torrent/1122940/Blackhat-2015-...          83\n",
      "2  http://1337x.to/torrent/1124395/Fast-and-Furio...          83\n",
      "3  http://1337x.to/torrent/1145504/Avengers-Age-o...          83\n",
      "4  http://1337x.to/torrent/1160078/Avengers-age-o...          83\n"
     ]
    }
   ],
   "source": [
    "print('Legitimate Data')\n",
    "print(legit_data[['url', 'url_length']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe96a0",
   "metadata": {},
   "source": [
    "**Insight**: As observed from our sample, phishing URLs can vary significantly in length, while legitimate URLs seem to exhibit more uniformity.\n",
    "\n",
    "**Phishing Data**: The lengths of phishing URLs from our sample vary widely. For instance, while the shortest URL in the displayed dataset has a length of 50 characters, the longest reaches up to 137 characters. This indicates that phishing URLs can come in various lengths, and there's no immediate pattern discernible from this small subset.\n",
    "\n",
    "**Legitimate Data**: The legitimate URLs from the sample seem to have a more consistent length, around 83 characters. This uniformity might be attributed to the specific source or platform from which these URLs are taken (in this case, all URLs seem to be from 1337x.to).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c00952",
   "metadata": {},
   "source": [
    "- **Presence of Sub-domains**\n",
    "\n",
    "The number of sub-domains can also be indicative. As mentioned, attackers often use multiple sub-domains to hide malicious intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cf153a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing Data\n",
      "                                                 url  sub_domain_count\n",
      "0  http://u1047531.cp.regruhosting.ru/acces-inges...                 3\n",
      "1  http://hoysalacreations.com/wp-content/plugins...                 1\n",
      "2  http://www.accsystemprblemhelp.site/checkpoint...                 3\n",
      "3  http://www.accsystemprblemhelp.site/login_atte...                 3\n",
      "4  https://firebasestorage.googleapis.com/v0/b/so...                 5\n",
      "Legitimate Data\n",
      "                                                 url  sub_domain_count\n",
      "0  http://1337x.to/torrent/1110018/Blackhat-2015-...                 1\n",
      "1  http://1337x.to/torrent/1122940/Blackhat-2015-...                 1\n",
      "2  http://1337x.to/torrent/1124395/Fast-and-Furio...                 1\n",
      "3  http://1337x.to/torrent/1145504/Avengers-Age-o...                 1\n",
      "4  http://1337x.to/torrent/1160078/Avengers-age-o...                 1\n"
     ]
    }
   ],
   "source": [
    "# Code to extract the sub-domain count\n",
    "print('Phishing Data')\n",
    "phish_data['sub_domain_count'] = phish_data['url'].apply(lambda x: x.count('.'))\n",
    "print(phish_data[['url', 'sub_domain_count']].head())\n",
    "\n",
    "\n",
    "print('Legitimate Data')\n",
    "legit_data['sub_domain_count'] = legit_data['url'].apply(lambda x: x.count('.'))\n",
    "print(legit_data[['url', 'sub_domain_count']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11828b7f",
   "metadata": {},
   "source": [
    "Insight: The varied range of sub-domains in phishing URLs contrasts with the consistent structure of the legitimate URLs in our sample.\n",
    "\n",
    "In this sample, phishing URLs exhibit a varied range of sub-domains, from 1 up to 5. Conversely, the legitimate URLs consistently show just 1 sub-domain. This disparity might indicate that phishing URLs often embed content within multiple sub-domains to obfuscate malicious intent, while the legitimate URLs from our chosen source tend to have a straightforward structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f5cc2",
   "metadata": {},
   "source": [
    "- **Presence of HTTPS**\n",
    "\n",
    "While HTTPS doesn't guarantee a website's legitimacy, the absence of it could be a red flag, given the modern web's emphasis on encrypted connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc78abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing Data - HTTPS Presence\n",
      "                                                 url  https_presence\n",
      "0  http://u1047531.cp.regruhosting.ru/acces-inges...           False\n",
      "1  http://hoysalacreations.com/wp-content/plugins...           False\n",
      "2  http://www.accsystemprblemhelp.site/checkpoint...           False\n",
      "3  http://www.accsystemprblemhelp.site/login_atte...           False\n",
      "4  https://firebasestorage.googleapis.com/v0/b/so...            True\n",
      "\n",
      "Legitimate Data - HTTPS Presence\n",
      "                                                 url  https_presence\n",
      "0  http://1337x.to/torrent/1110018/Blackhat-2015-...           False\n",
      "1  http://1337x.to/torrent/1122940/Blackhat-2015-...           False\n",
      "2  http://1337x.to/torrent/1124395/Fast-and-Furio...           False\n",
      "3  http://1337x.to/torrent/1145504/Avengers-Age-o...           False\n",
      "4  http://1337x.to/torrent/1160078/Avengers-age-o...           False\n"
     ]
    }
   ],
   "source": [
    "# Displaying the top rows of each dataset with the HTTPS presence feature\n",
    "print('Phishing Data - HTTPS Presence')\n",
    "phish_data['https_presence'] = phish_data['url'].str.startswith('https')\n",
    "print(phish_data[['url', 'https_presence']].head())\n",
    "\n",
    "print('\\nLegitimate Data - HTTPS Presence')\n",
    "legit_data['https_presence'] = legit_data['url'].str.startswith('https')\n",
    "print(legit_data[['url', 'https_presence']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cccfc2",
   "metadata": {},
   "source": [
    "Insight: Both phishing and legitimate datasets showcase URLs without HTTPS. This underlines that while HTTPS presence can be a good feature, it isn't solely indicative of a URL's legitimacy.\n",
    "\n",
    "From the displayed results, we observe that the majority of URLs in the phishing dataset sample, except the last one, do not start with HTTPS, implying a lack of a secured connection. Similarly, the sample from the legitimate dataset also predominantly lacks HTTPS. This provides an initial indication that the presence or absence of HTTPS is not an exclusive characteristic of phishing or legitimate sites, and further analysis is crucial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322f4b6",
   "metadata": {},
   "source": [
    "- **URL Tokenization**\n",
    "\n",
    "Tokenizing URLs can help in extracting meaningful terms, which can further aid during the modeling phase, especially if techniques like TF-IDF (Term Frequency-Inverse Document Frequency) are employed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd59137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for the first phishing URL based on TF-IDF score:\n",
      "3facd       0.397233\n",
      "u1047531    0.397233\n",
      "20200104    0.342948\n",
      "inges       0.342948\n",
      "t452        0.342948\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Top terms for the first legitimate URL based on TF-IDF score:\n",
      "2015    0.519832\n",
      "720p    0.519832\n",
      "dl      0.481934\n",
      "to      0.340997\n",
      "web     0.321426\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on phishing data and transform the URLs\n",
    "phish_vectorized = vectorizer.fit_transform(phish_data['url'])\n",
    "\n",
    "# Use the fitted vectorizer to transform the legitimate URLs\n",
    "legit_vectorized = vectorizer.transform(legit_data['url'])\n",
    "\n",
    "# Converting the sparse matrix to dense format for the first five phishing URLs\n",
    "phish_dense_sample = phish_vectorized[:5].todense()\n",
    "\n",
    "# Fetching the feature names (terms) from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Displaying the top terms for the first phishing URL based on the TF-IDF score\n",
    "df_phish_sample = pd.DataFrame(phish_dense_sample, columns=feature_names)\n",
    "top_terms_phish = df_phish_sample.iloc[0].nlargest(5)\n",
    "print(\"Top terms for the first phishing URL based on TF-IDF score:\")\n",
    "print(top_terms_phish)\n",
    "\n",
    "# Similarly, for legitimate URLs:\n",
    "legit_dense_sample = legit_vectorized[:5].todense()\n",
    "df_legit_sample = pd.DataFrame(legit_dense_sample, columns=feature_names)\n",
    "top_terms_legit = df_legit_sample.iloc[0].nlargest(5)\n",
    "print(\"\\nTop terms for the first legitimate URL based on TF-IDF score:\")\n",
    "print(top_terms_legit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a52fc",
   "metadata": {},
   "source": [
    "Insight: The terms with the highest TF-IDF scores differ between phishing and legitimate URLs, emphasizing distinct structures and contents between the two.\n",
    "\n",
    "The displayed terms represent the highest TF-IDF scores for the first URL in both the phishing and legitimate datasets. These terms provide insights into the components of the URLs deemed most significant by the TF-IDF vectorizer. For instance, in the phishing sample, terms like \"u1047531\" and \"regruhosting\" stand out, while in the legitimate sample, terms like \"1337x\" and \"torrent\" are predominant. This variation in terms between the two samples hints at the distinct structures and contents of phishing vs. legitimate URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86414aba",
   "metadata": {},
   "source": [
    "**URL Tokenization and Analysis**\n",
    "\n",
    "URLs can be split into various components like scheme, domain, subdomain, and path. By tokenizing URLs, we can extract information about its structure, which might reveal patterns typical for phishing URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c51e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'scheme': 'http', 'netloc': 'u1047531.cp.regr...\n",
      "1    {'scheme': 'http', 'netloc': 'hoysalacreations...\n",
      "2    {'scheme': 'http', 'netloc': 'www.accsystemprb...\n",
      "3    {'scheme': 'http', 'netloc': 'www.accsystemprb...\n",
      "4    {'scheme': 'https', 'netloc': 'firebasestorage...\n",
      "Name: tokenized_url, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def tokenize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return {\n",
    "        \"scheme\": parsed_url.scheme,\n",
    "        \"netloc\": parsed_url.netloc,\n",
    "        \"path\": parsed_url.path,\n",
    "        \"params\": parsed_url.params,\n",
    "        \"query\": parsed_url.query,\n",
    "        \"fragment\": parsed_url.fragment\n",
    "    }\n",
    "\n",
    "phish_data['tokenized_url'] = phish_data['url'].apply(tokenize_url)\n",
    "legit_data['tokenized_url'] = legit_data['url'].apply(tokenize_url)\n",
    "\n",
    "# Display some sample outputs:\n",
    "print(phish_data['tokenized_url'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7d4fc",
   "metadata": {},
   "source": [
    "**Lexical Analysis**\n",
    "\n",
    "Phishing URLs may have certain lexical patterns that distinguish them from legitimate URLs, like longer lengths or more subdomains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109553b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  url_length  \\\n",
      "0  http://u1047531.cp.regruhosting.ru/acces-inges...          66   \n",
      "1  http://hoysalacreations.com/wp-content/plugins...          88   \n",
      "2  http://www.accsystemprblemhelp.site/checkpoint...          50   \n",
      "3  http://www.accsystemprblemhelp.site/login_atte...          67   \n",
      "4  https://firebasestorage.googleapis.com/v0/b/so...         137   \n",
      "\n",
      "   num_special_chars  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  1  \n",
      "4                  1  \n"
     ]
    }
   ],
   "source": [
    "phish_data['url_length'] = phish_data['url'].apply(len)\n",
    "phish_data['num_special_chars'] = phish_data['url'].apply(lambda x: sum([c in set(['@', '&', '$']) for c in x]))\n",
    "\n",
    "# Displaying some sample outputs:\n",
    "print(phish_data[['url', 'url_length', 'num_special_chars']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bfc6fd",
   "metadata": {},
   "source": [
    "**Host-based and Domain Analysis**\n",
    "\n",
    "Information related to domain registration can be insightful. Domains that are very new or have been recently updated can be considered suspicious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3bda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-whois in c:\\users\\alish\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\alish\\anaconda3\\lib\\site-packages (from python-whois) (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-whois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d955cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c013d3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'whois' has no attribute 'WhoisCommandFailed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPywhoisError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mdomain_info\u001b[1;34m(domain)\u001b[0m\n\u001b[0;32m      6\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adding a delay of 1 second between requests\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m w \u001b[38;5;241m=\u001b[39m whois\u001b[38;5;241m.\u001b[39mwhois(domain)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\__init__.py:52\u001b[0m, in \u001b[0;36mwhois\u001b[1;34m(url, command, flags, executable)\u001b[0m\n\u001b[0;32m     51\u001b[0m     text \u001b[38;5;241m=\u001b[39m nic_client\u001b[38;5;241m.\u001b[39mwhois_lookup(\u001b[38;5;28;01mNone\u001b[39;00m, domain\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midna\u001b[39m\u001b[38;5;124m'\u001b[39m), flags)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m WhoisEntry\u001b[38;5;241m.\u001b[39mload(domain, text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\parser.py:205\u001b[0m, in \u001b[0;36mWhoisEntry.load\u001b[1;34m(domain, text)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m domain\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.com\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WhoisCom(domain, text)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m domain\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.net\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\parser.py:467\u001b[0m, in \u001b[0;36mWhoisCom.__init__\u001b[1;34m(self, domain, text)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo match for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PywhoisError(text)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mPywhoisError\u001b[0m: No match for \"FIREBASESTORAGE.GOOGLEAPIS.COM\".\r\n>>> Last update of whois database: 2023-09-12T03:41:35Z <<<\r\n\r\nNOTICE: The expiration date displayed in this record is the date the\r\nregistrar's sponsorship of the domain name registration in the registry is\r\ncurrently set to expire. This date does not necessarily reflect the expiration\r\ndate of the domain name registrant's agreement with the sponsoring\r\nregistrar.  Users may consult the sponsoring registrar's Whois database to\r\nview the registrar's reported date of expiration for this registration.\r\n\r\nTERMS OF USE: You are not authorized to access or query our Whois\r\ndatabase through the use of electronic processes that are high-volume and\r\nautomated except as reasonably necessary to register domain names or\r\nmodify existing registrations; the Data in VeriSign Global Registry\r\nServices' (\"VeriSign\") Whois database is provided by VeriSign for\r\ninformation purposes only, and to assist persons in obtaining information\r\nabout or related to a domain name registration record. VeriSign does not\r\nguarantee its accuracy. By submitting a Whois query, you agree to abide\r\nby the following terms of use: You agree that you may use this Data only\r\nfor lawful purposes and that under no circumstances will you use this Data\r\nto: (1) allow, enable, or otherwise support the transmission of mass\r\nunsolicited, commercial advertising or solicitations via e-mail, telephone,\r\nor facsimile; or (2) enable high volume, automated, electronic processes\r\nthat apply to VeriSign (or its computer systems). The compilation,\r\nrepackaging, dissemination or other use of this Data is expressly\r\nprohibited without the prior written consent of VeriSign. You agree not to\r\nuse electronic processes that are automated and high-volume to access or\r\nquery the Whois database except as reasonably necessary to register\r\ndomain names or modify existing registrations. VeriSign reserves the right\r\nto restrict your access to the Whois database in its sole discretion to ensure\r\noperational stability.  VeriSign may restrict or terminate your access to the\r\nWhois database for failure to abide by these terms of use. VeriSign\r\nreserves the right to modify these terms at any time.\r\n\r\nThe Registry database contains ONLY .COM, .NET, .EDU domains and\r\nRegistrars.\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m---> 14\u001b[0m phish_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m phish_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: domain_info(urlparse(x)\u001b[38;5;241m.\u001b[39mnetloc))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m---> 14\u001b[0m phish_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m phish_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: domain_info(urlparse(x)\u001b[38;5;241m.\u001b[39mnetloc))\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mdomain_info\u001b[1;34m(domain)\u001b[0m\n\u001b[0;32m      7\u001b[0m     w \u001b[38;5;241m=\u001b[39m whois\u001b[38;5;241m.\u001b[39mwhois(domain)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m whois\u001b[38;5;241m.\u001b[39mWhoisCommandFailed:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhoisCommandFailed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'whois' has no attribute 'WhoisCommandFailed'"
     ]
    }
   ],
   "source": [
    "import whois\n",
    "import time\n",
    "\n",
    "def domain_info(domain):\n",
    "    try:\n",
    "        time.sleep(1)  # Adding a delay of 1 second between requests\n",
    "        w = whois.whois(domain)\n",
    "        return w\n",
    "    except whois.WhoisCommandFailed:\n",
    "        return \"WhoisCommandFailed\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "phish_data['domain_info'] = phish_data['url'].apply(lambda x: domain_info(urlparse(x).netloc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11002] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11002] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11002] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "# Using WHOIS for domain information (you may need to install the python-whois package)\n",
    "import whois\n",
    "\n",
    "def domain_info(domain):\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        return w\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "phish_data['domain_info'] = phish_data['url'].apply(lambda x: domain_info(urlparse(x).netloc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ed98c",
   "metadata": {},
   "source": [
    "**Content-based Analysis**\n",
    "\n",
    "Objective:\n",
    "Analyze the content a URL points to for indicators of phishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.text\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9564a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take time and might not be practical for a huge dataset.\n",
    "# phish_data['content'] = phish_data['url'].apply(get_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8439ab",
   "metadata": {},
   "source": [
    "**Third-party Data Enrichment**\n",
    "\n",
    "Objective:\n",
    "Utilize third-party services to gather more detailed insights about URLs or domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ef7f3",
   "metadata": {},
   "source": [
    "## **Data Pre-processing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b049715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded your data into phish_data and legit_data\n",
    "\n",
    "# Step 1: Randomly select a subset of legitimate URLs to match the phishing count\n",
    "subset_legit_data = legit_data.sample(n=len(phish_data))\n",
    "\n",
    "# Step 2: Assign labels\n",
    "phish_data['label'] = 1  # phishing\n",
    "subset_legit_data['label'] = 0  # legitimate\n",
    "\n",
    "# Step 3: Concatenate\n",
    "combined_data = pd.concat([phish_data, subset_legit_data], axis=0)\n",
    "\n",
    "# Step 4: Shuffle\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "print(\"First few rows of the processed dataset:\")\n",
    "display(combined_data.head())\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"\\nShape of the dataset: {combined_data.shape}\")\n",
    "\n",
    "# Display counts of each label\n",
    "print(\"\\nCounts of each label:\")\n",
    "display(combined_data['label'].value_counts())\n",
    "\n",
    "# Display a simple statistic: mean length of URLs (assuming a 'url' column exists)\n",
    "print(f\"\\nAverage URL length: {combined_data['url'].apply(len).mean():.2f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3cbab",
   "metadata": {},
   "source": [
    "**Align Columns Before Concatenating**\n",
    "    \n",
    "Ensure that both datasets have the exact same columns in the same order. If they have different columns, decide on a common set of columns to use, or fill in missing columns in each dataset with placeholder values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Making sure both dataframes have the same columns\n",
    "missing_cols_legit = set(phish_data.columns) - set(legit_data.columns)\n",
    "missing_cols_phish = set(legit_data.columns) - set(phish_data.columns)\n",
    "\n",
    "# Adding missing columns to both dataframes\n",
    "for col in missing_cols_legit:\n",
    "    legit_data[col] = np.nan\n",
    "for col in missing_cols_phish:\n",
    "    phish_data[col] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64903232",
   "metadata": {},
   "source": [
    "1. **Combining Datasets**\n",
    "\n",
    "Before we can pre-process our data, we need to merge the two datasets (phishing and legitimate) into a single dataframe.\n",
    "\n",
    "1.1 Combine Datasets\n",
    "\n",
    "We'll begin by concatenating the phishing and legitimate datasets. Afterwards, we'll shuffle them to ensure randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c195eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate and shuffle\n",
    "data = pd.concat([phish_data, legit_data]).sample(frac=1).reset_index(drop=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e51c6",
   "metadata": {},
   "source": [
    "1. Combining and Scaling Features\n",
    "1.1 Combine Datasets\n",
    "Combine phishing and legitimate datasets and shuffle them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.concat([phish_data, legit_data]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extracting the feature matrix 'X' and target 'y'\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb393874",
   "metadata": {},
   "source": [
    "## **Data Pre-processing**\n",
    "\n",
    "The purpose of data pre-processing is to convert the raw data into a clean data set, ensuring the quality and integrity of the data before feeding it into the modeling algorithms.\n",
    "\n",
    "- **Feature Scaling**\n",
    "        \n",
    "Given the disparity in magnitude between different features (e.g., URL length versus TF-IDF scores), scaling becomes essential, especially if you intend to use distance-based algorithms.\n",
    "\n",
    "Feature Scaling \n",
    "Identifying the Need for Scaling: Before delving into the actual scaling process, we identified the necessity for this step. Given the range and type of the features extracted, such as URL length and TF-IDF scores, some features could disproportionately influence the outcome of distance-based machine learning algorithms due to their magnitude. Thus, scaling was deemed essential.\n",
    "\n",
    "Choosing an Appropriate Scaling Technique: Among various scaling techniques available (like Min-Max scaling, Robust scaling, etc.), we chose the Standard Scaler. This scaler standardizes the features by removing the mean and scaling to unit variance.\n",
    "\n",
    "Initialization and Application:\n",
    "\n",
    "We initialized the StandardScaler from the sklearn.preprocessing module.\n",
    "The scaler was then fit using the phishing dataset. This means the scaler calculated the mean and standard deviation from the phishing dataset's features.\n",
    "Using this fit scaler, we transformed both the phishing and legitimate datasets to get standardized values.\n",
    "Validation and Verification:\n",
    "\n",
    "After scaling, we displayed the top rows of both datasets to verify the success of the transformation.\n",
    "The scaled features for both datasets are observed to be around a mean of zero, verifying the proper functioning of the scaler.\n",
    "Observations from Scaled Data: From the scaled data, certain patterns and variations between the phishing and legitimate datasets were observed, which could potentially assist in better differentiation during modeling.\n",
    "\n",
    "By implementing feature scaling, we have ensured that no particular feature would unduly influence any model, especially those sensitive to feature magnitudes. This paves the way for the next steps in the machine learning pipeline, such as model selection and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d23365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python code to scale the phishing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_phish = StandardScaler()\n",
    "phish_data_numeric = data[data['label'] == 'phishing'].select_dtypes(include=[np.number])\n",
    "\n",
    "phish_data_scaled = scaler_phish.fit_transform(phish_data_numeric)\n",
    "\n",
    "\n",
    "# Display the first few rows of the scaled phishing data\n",
    "print(\"Scaled Phishing Data:\")\n",
    "print(pd.DataFrame(phish_data_scaled, columns=phish_data_numeric.columns).head())\n",
    "\n",
    "# Display the first few rows of the scaled legitimate data\n",
    "print(\"\\nScaled Legitimate Data:\")\n",
    "print(pd.DataFrame(legit_data_scaled, columns=legit_data_numeric.columns).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2060f",
   "metadata": {},
   "source": [
    "Observation: The scaled data reveals some interesting patterns. For both phishing and legitimate datasets, the url_length, sub_domain_count, and https_presence features have been standardized such that they have values around a mean of zero. Notably, the legitimate data appears more consistent in its scaled values, especially in terms of sub-domains and HTTPS presence. In contrast, the phishing data displays more variability. This difference in consistency and variability can be a potential differentiator when classifying URLs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77323108",
   "metadata": {},
   "source": [
    "- **Data Balancing**\n",
    "\n",
    "The imbalance between phishing and legitimate URLs can lead to biased models. Let's first check the class distribution and then decide on the method for balancing.\n",
    "\n",
    "In classification tasks, if one class outnumbers the other class (or classes), a model can be heavily biased towards the majority class, leading to poor performance on the minority class. This phenomenon is commonly seen in fraud detection, spam filtering, and, in our case, phishing URL detection.\n",
    "\n",
    "Explanation:\n",
    "Over-sampling is a technique where you generate more samples in the minority class, usually by duplicating or creating synthetic samples using methods like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Under-sampling involves reducing the number of samples in the majority class, but it can result in loss of data.\n",
    "\n",
    "\n",
    "- **Update Both Libraries**\n",
    "\n",
    "Begin by updating both imblearn and sklearn to their latest versions. This might resolve any version incompatibility issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e113f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip\n",
    "\n",
    "\n",
    "!pip install imbalanced-learn --upgrade\n",
    "\n",
    "\n",
    "!pip install scikit-learn --upgrade\n",
    "\n",
    "\n",
    "pip cache purge\n",
    "\n",
    "!pip install imbalanced-learn --upgrade --no-cache-dir\n",
    "!pip install scikit-learn --upgrade --no-cache-dir\n",
    "\n",
    "\n",
    "pip install imbalanced-learn --upgrade -i https://pypi.python.org/simple/\n",
    "\n",
    "\n",
    "!pip install -U imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b964850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import sklearn\n",
    "\n",
    "print(\"Imbalanced Learn Version:\", imblearn.__version__)\n",
    "print(\"Scikit Learn Version:\", sklearn.__version__)\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Instantiate SMOTE with a specific random state for reproducibility\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Resample the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "!pip install imbalanced-learn==X.X.X\n",
    "!pip install scikit-learn==Y.Y.Y\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d854c3",
   "metadata": {},
   "source": [
    "If there's a significant imbalance, consider techniques like:\n",
    "\n",
    "a. Oversampling:\n",
    "Increasing the count of the minority class by replicating samples.\n",
    "\n",
    "b. Undersampling:\n",
    "Reducing the count of the majority class by randomly removing samples.\n",
    "\n",
    "c. SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "Generating synthetic samples for the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea01fb",
   "metadata": {},
   "source": [
    "**Splitting**\n",
    "\n",
    "Partition the data into training, validation, and testing sets.\n",
    "\n",
    "Splitting the dataset into training and testing subsets is a crucial step in building and evaluating machine learning models. This ensures that we have a set of unseen data to validate the performance of our model. Typically, we use 70-80% of the data for training and the remaining 20-30% for testing.\n",
    "\n",
    "Explanation:\n",
    "The training set is used to train the machine learning model. It's like giving the model a set of example problems to learn from.\n",
    "\n",
    "The testing set is used to evaluate the model's performance. This set is not shown to the model during training. It's like giving the model a quiz on what it has learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combining the data for splitting\n",
    "combined_data = pd.concat([phish_data_scaled, legit_data_scaled], axis=0)\n",
    "\n",
    "# Labels: 1 for phishing and 0 for legitimate\n",
    "labels = np.concatenate([np.ones(phish_data_scaled.shape[0]), np.zeros(legit_data_scaled.shape[0])])\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1155c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1524585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d2212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
